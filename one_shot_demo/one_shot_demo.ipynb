{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from os import walk\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/kmmbd/one_shot_demo\" target=\"_blank\">https://app.wandb.ai/kmmbd/one_shot_demo</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/kmmbd/one_shot_demo/runs/2fxrt9co\" target=\"_blank\">https://app.wandb.ai/kmmbd/one_shot_demo/runs/2fxrt9co</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/kmmbd/one_shot_demo\" target=\"_blank\">https://app.wandb.ai/kmmbd/one_shot_demo</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/kmmbd/one_shot_demo/runs/2cnq7c0w\" target=\"_blank\">https://app.wandb.ai/kmmbd/one_shot_demo/runs/2cnq7c0w</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/kmmbd/one_shot_demo/runs/2cnq7c0w"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='one_shot_demo')\n",
    "wandb.init(config={\"epochs\": 20, \"batch_size\": 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotDataset(Dataset):\n",
    "    def __init__(self, categories, root_dir, setSize, transform = None):\n",
    "        self.categories = categories\n",
    "        self.root_dir = root_dir\n",
    "        self.setSize = setSize\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.setSize\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img1 = None\n",
    "        img2 = None\n",
    "        label1 = None\n",
    "        if idx % 2 == 0: # select same character\n",
    "            category = random.choice(categories)\n",
    "            character = random.choice(category[1])\n",
    "            imgDir = root_dir + category[0] + '/' + character\n",
    "            img1Name = random.choice(os.listdir(imgDir))\n",
    "            img2Name = random.choice(os.listdir(imgDir))\n",
    "            img1 = Image.open(imgDir + '/' + img1Name)\n",
    "            img2 = Image.open(imgDir + '/' + img2Name)\n",
    "            label = 1.0 # assign label as true\n",
    "            \n",
    "        else: # select different characters \n",
    "            category1, category2 = random.choice(categories), random.choice(categories)\n",
    "            character1, character2 = random.choice(category1[1]), random.choice(category2[1])\n",
    "            imgDir1 = root_dir + category1[0] + '/' + character1\n",
    "            imgDir2 = root_dir + category2[0] + '/' + character2\n",
    "            img1Name = random.choice(os.listdir(imgDir1))\n",
    "            img2Name = random.choice(os.listdir(imgDir2))\n",
    "            while img1Name == img2Name:\n",
    "                img2Name = random.choice(os.listdir(imgDir2))\n",
    "            label = 0.0 # assign label as false \n",
    "            img1 = Image.open(imgDir1 + '/' + img1Name)\n",
    "            img2 = Image.open(imgDir2 + '/' + img2Name)\n",
    "            \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "            \n",
    "        return img1, img2, torch.from_numpy(np.array([label], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NWayOneShotEvalSet(Dataset):\n",
    "    \n",
    "    def __init__(self, categories, root_dir, setSize, numWay, transform = None):\n",
    "        self.categories = categories\n",
    "        self.root_dir = root_dir\n",
    "        self.setSize = setSize\n",
    "        self.numWay = numWay\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.setSize\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # find one main image\n",
    "        category = random.choice(categories)\n",
    "        character = random.choice(categories[1])\n",
    "        imgDir = root_dir + category[0] + '/' + character\n",
    "        imgName = random.choice(os.listdir(imgDir))\n",
    "        mainImg = Image.open(imgDir + '/' + imgName)\n",
    "        if self.transform:\n",
    "            mainImg = self.transform(mainImg)\n",
    "            \n",
    "        # find n numbers of distinct images, 1 in same set as main\n",
    "        testSet = []\n",
    "        label = np.random.randint(self.numWay)\n",
    "        for i in range(self.numWay):\n",
    "            testImgDir = imgDir\n",
    "            testImgName = ''\n",
    "            if i == label:\n",
    "                testImgName = random.choice(os.listdir(imgDir))\n",
    "            else:\n",
    "                testCategory = random.choice(categories)\n",
    "                testCharacter = random.choice(testCategory[1])\n",
    "                testImgDir = root_dir + testCategory[0] + '/' + testCharacter\n",
    "                while testImgDir == imgDir:\n",
    "                    testImgDir = root_dir + testCategory[0] + '/' + testCharacter\n",
    "                testImgName = random.choice(os.listdir(testImgDir))\n",
    "            \n",
    "            testImg = Image.open(testImgDir + '/' + testImgName)\n",
    "            if self.transform:\n",
    "                testImg = self.transform(testImg)\n",
    "                \n",
    "            testSet.append(testImg)\n",
    "        \n",
    "        return mainImg, testSet, torch.from_numpy(np.array([label], dytpe = int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #conv2d(input_channels, output_channels, kernel_size)\n",
    "        self.conv1 = nn.Conv2d(1, 64, 10)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 7)\n",
    "        self.conv3 = nn.Conv2d(128, 128, 4)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(256*6*6, 4096)\n",
    "        self.fcOut = nn.Linear(4096, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def convs(self, x):\n",
    "        # out_dim = in_dim - kernel_size + 1\n",
    "        #1, 105, 105\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # 64, 96, 96\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        # 64, 48, 48\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        # 128, 42, 42\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convs(x1)\n",
    "        x1 = x1.view(-1, 256*6*6)\n",
    "        x1 = self.sigmoid(self.fc1(x1))\n",
    "        x2 = self.convs(x2)\n",
    "        x2 = x2.view(-1, 256*6*6)\n",
    "        x2 = self.sigmoid(self.fc1(x2))\n",
    "        x = torch.abs(x1 - x2)\n",
    "        x = self.fcOut(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation after every epoch\n",
    "def train(model, train_loader, val_loader, num_epochs, criterion, save_name):\n",
    "    best_val_loss = float(\"Inf\") \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    cur_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        print(\"Starting epoch \" + str(epoch+1))\n",
    "        for img1, img2, labels in train_loader:\n",
    "            \n",
    "            # Forward\n",
    "            img1 = img1.to(device)\n",
    "            img2 = img2.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(img1, img2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        wandb.log({'epoch': epoch, 'train_loss': avg_train_loss})\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for img1, img2, labels in val_loader:\n",
    "                img1 = img1.to(device)\n",
    "                img2 = img2.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(img1, img2)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "        avg_val_loss = val_running_loss / len(val_loader)\n",
    "        wandb.log({'epoch': epoch, 'validation_loss': loss})\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print('Epoch [{}/{}],Train Loss: {:.4f}, Valid Loss: {:.8f}'\n",
    "            .format(epoch+1, num_epochs, avg_train_loss, avg_val_loss))\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_checkpoint(save_name, model, optimizer, best_val_loss)\n",
    "    \n",
    "    print(\"Finished Training\")  \n",
    "    return train_losses, val_losses  \n",
    "\n",
    "# evaluation metrics\n",
    "def eval(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        print('Starting Iteration')\n",
    "        count = 0\n",
    "        for mainImg, imgSets, label in test_loader:\n",
    "            mainImg = mainImg.to(device)\n",
    "            predVal = 0\n",
    "            pred = -1\n",
    "            for i, testImg in enumerate(imgSets):\n",
    "                testImg = testImg.to(device)\n",
    "                output = model(mainImg, testImg)\n",
    "                if output > predVal:\n",
    "                    pred = i\n",
    "                    predVal = output\n",
    "            label = label.to(device)\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "            count += 1\n",
    "            if count % 20 == 0:\n",
    "                print(\"Current Count is: {}\".format(count))\n",
    "                print('Accuracy on n way: {}'.format(correct/count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './images_background/'\n",
    "categories = [[folder, os.listdir(root_dir + folder)] for folder in os.listdir(root_dir)  if not folder.startswith('.') ]\n",
    "# print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# choose a training dataset size and further divide it into train and validation set 80:20\n",
    "dataSize = 10000 # self-defined dataset size\n",
    "TRAIN_PCT = 0.8 # percentage of entire dataset for training\n",
    "train_size = int(dataSize * TRAIN_PCT)\n",
    "val_size = dataSize - train_size\n",
    "\n",
    "transformations = transforms.Compose(\n",
    "    [transforms.ToTensor()]) \n",
    "\n",
    "omniglotDataset = OmniglotDataset(categories, root_dir, dataSize, transformations)\n",
    "train_set, val_set = random_split(omniglotDataset, [train_size, val_size])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, num_workers=0, shuffle=True)\n",
    "\n",
    "# create the test set for final testing\n",
    "testSize = 5000 \n",
    "numWay = 20\n",
    "test_set = NWayOneShotEvalSet(categories, root_dir, testSize, numWay, transformations)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 1, num_workers = 0, shuffle=True)\n",
    "\n",
    "count0 = 0\n",
    "count1 = 0\n",
    "for img1, img2, label in train_loader:\n",
    "    print()\n",
    "    if label[0] == 1.0:\n",
    "        print(img1[0])\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(img1[0][0])\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(img2[0][0])\n",
    "        # print(label)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show for test input\n",
    "count = 0\n",
    "for mainImg, imgset, label in test_loader:\n",
    "    # print(len(imgset))\n",
    "    # print(label)\n",
    "    # print(imgset.shape)\n",
    "    if label != 1:\n",
    "        for count, img in enumerate(imgset):\n",
    "            plt.subplot(1, len(imgset)+1, count+1)\n",
    "            plt.imshow(img[0][0])\n",
    "          # print(img.shape)\n",
    "        print(mainImg.shape)\n",
    "        plt.subplot(1, len(imgset)+1, len(imgset)+1)\n",
    "        plt.imshow(mainImg[0][0])\n",
    "        count += 1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "1.5.0\n",
      "The model architecture:\n",
      "\n",
      " Net(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(10, 10), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "  (fcOut): Linear(in_features=4096, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "The model has 38,952,897 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "#creating the original network and couting the paramenters of different networks\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.__version__)\n",
    "# print(dir(torch.nn))\n",
    "siameseBaseLine = Net()\n",
    "siameseBaseLine = siameseBaseLine.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    temp = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model architecture:\\n\\n', model)\n",
    "    print(f'\\nThe model has {temp:,} trainable parameters')\n",
    "    \n",
    "count_parameters(siameseBaseLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load checkpoints\n",
    "\n",
    "def save_checkpoint(save_path, model, optimizer, val_loss):\n",
    "    if save_path==None:\n",
    "        return\n",
    "    save_path = save_path \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'val_loss': val_loss}\n",
    "\n",
    "    torch.save(state_dict, save_path)\n",
    "\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(model, optimizer):\n",
    "    save_path = f'siameseNet-batchnorm50.pt'\n",
    "    state_dict = torch.load(save_path)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    val_loss = state_dict['val_loss']\n",
    "    print(f'Model loaded from <== {save_path}')\n",
    "    \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch [1/20],Train Loss: 0.5459, Valid Loss: 0.46156218\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 2\n",
      "Epoch [2/20],Train Loss: 0.4484, Valid Loss: 0.41039874\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 3\n",
      "Epoch [3/20],Train Loss: 0.3863, Valid Loss: 0.33398706\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 4\n",
      "Epoch [4/20],Train Loss: 0.3495, Valid Loss: 0.33111261\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 5\n",
      "Epoch [5/20],Train Loss: 0.3235, Valid Loss: 0.31446364\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 6\n",
      "Epoch [6/20],Train Loss: 0.3017, Valid Loss: 0.31469410\n",
      "Starting epoch 7\n",
      "Epoch [7/20],Train Loss: 0.2977, Valid Loss: 0.28499631\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 8\n",
      "Epoch [8/20],Train Loss: 0.2777, Valid Loss: 0.27618047\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 9\n",
      "Epoch [9/20],Train Loss: 0.2655, Valid Loss: 0.24904275\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 10\n",
      "Epoch [10/20],Train Loss: 0.2567, Valid Loss: 0.24351293\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 11\n",
      "Epoch [11/20],Train Loss: 0.2610, Valid Loss: 0.24083088\n",
      "Model saved to ==> siameseNet-batchnorm50.pt\n",
      "Starting epoch 12\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "\n",
    "optimizer = optim.Adam(siameseBaseLine.parameters(), lr = 0.0006)\n",
    "num_epochs = 20\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "save_path = 'siameseNet-batchnorm50.pt'\n",
    "wandb.watch(siameseBaseLine)\n",
    "train_losses, val_losses = train(siameseBaseLine, train_loader, val_loader, num_epochs, criterion, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
